Index: wav2vec2_patch/modeling_wav2vec2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/wav2vec2_patch/modeling_wav2vec2.py b/wav2vec2_patch/modeling_wav2vec2.py
--- a/wav2vec2_patch/modeling_wav2vec2.py	
+++ b/wav2vec2_patch/modeling_wav2vec2.py	(date 1727361954864)
@@ -607,6 +607,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
+        attn_weights_no_softmax = attn_weights.clone()  # Erf
         attn_weights = nn.functional.softmax(attn_weights, dim=-1)
 
         if layer_head_mask is not None:
@@ -623,10 +624,13 @@
             # make sure that attn_weights keeps its gradient.
             # In order to do so, attn_weights have to be reshaped
             # twice and have to be reused in the following
-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)
+            # attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)  # Erf_c
+            # attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)  # Erf_c
+            attn_weights_reshaped_no_softmax = attn_weights_no_softmax.view(bsz, self.num_heads, tgt_len, src_len)  # Erf
+            attn_weights_no_softmax = attn_weights_reshaped_no_softmax.view(bsz * self.num_heads, tgt_len, src_len)  # Erf
         else:
-            attn_weights_reshaped = None
+            # attn_weights_reshaped = None  # Erf_c
+            attn_weights_reshaped_no_softmax = None
 
         attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
 
@@ -647,7 +651,8 @@
 
         attn_output = self.out_proj(attn_output)
 
-        return attn_output, attn_weights_reshaped, past_key_value
+        # return attn_output, attn_weights_reshaped, past_key_value  # Erf_c
+        return attn_output, attn_weights_reshaped_no_softmax, past_key_value  # Erf
 
 
 # Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->Wav2Vec2
