Index: modified_sources/modeling_wav2vec2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/modified_sources/modeling_wav2vec2.py b/modified_sources/modeling_wav2vec2.py
--- a/modified_sources/modeling_wav2vec2.py	
+++ b/modified_sources/modeling_wav2vec2.py	(date 1717604965730)
@@ -616,6 +616,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
+        attn_weights_no_softmax = attn_weights.clone()  # Erf
         attn_weights = nn.functional.softmax(attn_weights, dim=-1)
 
         if layer_head_mask is not None:
@@ -632,8 +633,10 @@
             # make sure that attn_weights keeps its gradient.
             # In order to do so, attn_weights have to be reshaped
             # twice and have to be reused in the following
-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)
+            # attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)  # Erf c
+            # attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)   # Erf c
+            attn_weights_reshaped_no_softmax = attn_weights_no_softmax.view(bsz, self.num_heads, tgt_len, src_len)  # Erf
+            attn_weights_no_softmax = attn_weights_reshaped_no_softmax.view(bsz * self.num_heads, tgt_len, src_len)  # Erf
         else:
             attn_weights_reshaped_no_softmax = None
 
@@ -656,7 +659,8 @@
 
         attn_output = self.out_proj(attn_output)
 
-        return attn_output, attn_weights_reshaped, past_key_value
+        # return attn_output, attn_weights_reshaped, past_key_value  # Erf c
+        return attn_output, attn_weights_reshaped_no_softmax, past_key_value  # Erf
 
 
 # Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->Wav2Vec2
